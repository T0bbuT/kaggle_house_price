{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2647e24d-5b7a-4cbf-86d7-f6d0ed708137",
   "metadata": {},
   "source": [
    "# 1. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac5967-b740-4bd0-9f27-d7d98e98a0a1",
   "metadata": {},
   "source": [
    "## データセット読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa5512-4763-42e4-aa93-caea2561059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import seaborn as sns\n",
    "# from scipy.stats import norm    #確率分布関連\n",
    "# from scipy import stats    #統計関連\n",
    "# from scipy.special import boxcox1p\n",
    "from scipy import stats\n",
    "from scipy import special\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "df_train = pd.read_csv(\"train_test_submission/train.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"train_test_submission/test.csv\", index_col=0)\n",
    "\n",
    "print(f\"df_train.shape: {df_train.shape}\")\n",
    "display(df_train.head(3))\n",
    "print(f\"df_train.shape: {df_test.shape}\")\n",
    "display(df_test.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba622c-197a-4b49-9381-748ecb2c73d7",
   "metadata": {},
   "source": [
    "## ydata_profilingを使う場合（普段はコメントアウト。時間かかるので注意）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783da395-454d-4026-96e6-802f35869690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ydata_profilingを使う場合\n",
    "\n",
    "# from ydata_profiling import ProfileReport\n",
    "# profile = ProfileReport(df_train, minimal=True)\n",
    "# profile.to_file(\"ydata_profiling/kaggle_houseprices.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72971379-e132-4ea9-aa3c-1f9ba1999983",
   "metadata": {},
   "source": [
    "## 目的変数(SalePrice)の特徴把握"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf053bc5-305a-48d4-b6e1-a7aff98f4499",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_train[\"SalePrice\"]\n",
    "\n",
    "# 目的変数（SalePrice）の基本統計量を表示\n",
    "print(\"-\" * 10, \"describe\", \"-\" * 10)\n",
    "print(round(data.describe(), 1))\n",
    "\n",
    "# 歪度と尖度\n",
    "skewness = data.skew()\n",
    "kurtosis = data.kurtosis()\n",
    "print(f\"歪度(Skewness: {skewness})\")\n",
    "print(f\"尖度(Kurtosis: {kurtosis})\")\n",
    "\n",
    "# グラフ描写\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "sns.histplot(data, stat=\"density\", kde=True, ax=ax[0])\n",
    "ax[0].set_title(\"ヒストグラムと正規分布\")\n",
    "ax[0].tick_params(axis=\"x\", labelsize=8)  # 軸の文字サイズ変更\n",
    "\n",
    "# 正規分布を重ねて表示（黒線）\n",
    "xmin, xmax = ax[0].get_xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = stats.norm.pdf(x, np.mean(data), np.std(data))\n",
    "ax[0].plot(x, p, \"k\", linewidth=1)\n",
    "\n",
    "# 正規確率プロットを表示するサブプロット\n",
    "res = stats.probplot(data, plot=ax[1])\n",
    "ax[1].set_title(\"正規確率プロット\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ・歪度 (skewness) の絶対値が 1.88 と高い\n",
    "# ・ヒストグラムの正規分布線 (黒線) と KDE のズレが発生\n",
    "# ・QQプロット線 (青線) が正規分布線 (赤線) からズレが発生\n",
    "# 以上３点からSalePriceが正規分布に対して歪んでいることが分かる。\n",
    "# 正規分布から歪んでいると、機械学習の精度の低下につながる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d289916-ce86-444c-9c25-1fa3627fcc4a",
   "metadata": {},
   "source": [
    "## SalePriceと他の特徴量との関係"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af6107-a981-4e3c-8696-634142c2d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GrLivArea\t地上（地上）の生活エリアの平方フィート。\n",
    "ax = sns.scatterplot(data=df_train, x=\"GrLivArea\", y=\"SalePrice\")\n",
    "\n",
    "# 右下に2つ大きく傾向から外れた値がある。\n",
    "# この２点は住居エリアが広くて、住宅価格が安い\n",
    "# おそらく郊外のエリア。外れ値の候補として考えておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c91780-d4b4-402b-947d-094681303746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YearBuilt 建築年\n",
    "plt.figure(figsize=(16, 5))\n",
    "ax = sns.boxplot(data=df_train, x=\"YearBuilt\", y=\"SalePrice\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "# 意外とあまり傾向が見えない？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d18121-506f-4230-a413-617d68c24c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OverallQual：\t全体的な材料と仕上げの品質。\n",
    "ax = sns.boxplot(data=df_train, x=\"OverallQual\", y=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab154c97-a904-4536-a8ae-e77b78cb3aa9",
   "metadata": {},
   "source": [
    "## 相関係数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d2158-ed5a-4f1c-87ef-508991862988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべての特徴量について\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    df_train.corr(numeric_only=True),\n",
    "    cmap=\"viridis\",\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    annot_kws={\"fontsize\": 6},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f94bb5-35b6-4fd8-8bb9-77b03af19ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SalePriceとの相関がある特徴量のみについて\n",
    "\n",
    "threshold = 0.4  # 相関係数の閾値\n",
    "high_corr_cols = (\n",
    "    df_train.corr(numeric_only=True)[\"SalePrice\"][\n",
    "        abs(df_train.corr(numeric_only=True)[\"SalePrice\"]) >= threshold\n",
    "    ]\n",
    "    .sort_values(ascending=False)\n",
    "    .index\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    df_train[high_corr_cols].corr(),\n",
    "    cmap=\"viridis\",\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    annot_kws={\"fontsize\": 10},\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 方針：特徴量同士で相関係数が高い項目に関しては、片方を除外、もしくは、別の特徴量を検討\n",
    "# GarageCarsが多ければ、GarageAreaが多いのも必然のためGarageAreaは除外\n",
    "# GrLivArea(地上の生活エリア)が広ければ、TotRmsAbvGrd(地上の部屋数)も多くなるのが一般的なので、TotRmsAbvGrdは除外\n",
    "# 車庫と建物が同時に建設されることが一般的なので、YearBuiltとGarageYrBltが相関が高くなっていると考えられるため、GarageYrBitは除外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot\n",
    "threshold = 0.6  # 相関係数の閾値\n",
    "high_corr_cols = (\n",
    "    df_train.corr(numeric_only=True)[\"SalePrice\"][\n",
    "        abs(df_train.corr(numeric_only=True)[\"SalePrice\"]) >= threshold\n",
    "    ]\n",
    "    .sort_values(ascending=False)\n",
    "    .index\n",
    ")\n",
    "\n",
    "sns.set(\n",
    "    font=\"IPAexGothic\"\n",
    ")  # ここでフォントを指定しておかないと、後のグラフ描画にて日本語が文字化けする\n",
    "sns.pairplot(df_train[high_corr_cols], height=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc20a6",
   "metadata": {},
   "source": [
    "# 2. 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e9343-91a3-4f3e-8e09-1918343df6aa",
   "metadata": {},
   "source": [
    "## 欠損値処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1524f-961c-4755-b8f2-a3da5293311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameを結合\n",
    "df_all_data = pd.concat([df_train, df_test])\n",
    "\n",
    "# 欠損値の数を計算\n",
    "missing_values_count = df_all_data.isna().sum()\n",
    "missing_values_table = pd.DataFrame(\n",
    "    {\n",
    "        \"Missing_total\": missing_values_count,\n",
    "        \"Percent (%)\": round(missing_values_count / len(df_all_data) * 100, 2),\n",
    "    }\n",
    ")\n",
    "\n",
    "# 欠損値の割合順で並べ替え\n",
    "df_missing = missing_values_table[\n",
    "    missing_values_table[\"Missing_total\"] > 0\n",
    "].sort_values(by=\"Missing_total\", ascending=False)\n",
    "# 処理法を決めるのに使う作業用csvを出力\n",
    "df_data_description = pd.read_csv(\n",
    "    \"data_description/data_description_日本語.csv\", index_col=0\n",
    ")\n",
    "df_missing_value_processing = pd.concat([df_missing, df_data_description], axis=1)\n",
    "# encoding=\"utf-8_sig\"を付けることでexcelで開いたときの文字化けを回避\n",
    "df_missing_value_processing.to_csv(\n",
    "    \"missing_value_processing/missing_value_processing_欠損値処理.csv\",\n",
    "    encoding=\"utf-8_sig\",\n",
    ")\n",
    "\n",
    "display(df_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b664281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"LotFrontage\"について\n",
    "plt.figure(figsize=(20, 4))\n",
    "sns.boxplot(data=df_all_data, x=\"Neighborhood\", y=\"LotFrontage\")\n",
    "plt.show()\n",
    "# 下の結果から、地域ごとに\"LotFrontage\"の値にばらつきがあることが分かる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d331ccd-15f8-4c23-883c-6c2224eb4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各地域\"Neighborhood\"の\"LotFrontage\"の中央値\n",
    "df_group_LotFrontage = df_all_data.groupby(by=\"Neighborhood\")[\"LotFrontage\"].agg(\n",
    "    \"median\"\n",
    ")\n",
    "\n",
    "\n",
    "def fillnaLot(row):\n",
    "    \"\"\"\n",
    "    ある1つの住宅データについて、\"LotFrontage\"列の値が欠損している場合はそのデータの地域（\"Neighborhood\"）の\"LotFrontage\"の中央値で補完。\n",
    "    欠損していない場合、元の値をそのまま返す。\n",
    "\n",
    "    Parameter\n",
    "    ----------\n",
    "    row : pandas.core.frame.DataFrame\n",
    "        欠損値処理をしたいデータセット\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    欠損値処理がされたLotFrontage行が返ってくる\n",
    "    \"\"\"\n",
    "    # もし\"LotFrontage\"が欠損値（NaN）または空文字列の場合\n",
    "    # NaN\t「何もないよー」な意味をあらわす単語\n",
    "    # 空文字列\t長さ0文字の文字列のこと\n",
    "    # でもisnaは空文字列\"\"を欠損値として認識するはずだけど…？取り敢えず教材通りに書いておく\n",
    "\n",
    "    if pd.isna(row[\"LotFrontage\"]) or row[\"LotFrontage\"] == \"\":\n",
    "        return df_group_LotFrontage[row[\"Neighborhood\"]]\n",
    "    else:\n",
    "        return row[\"LotFrontage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a210b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## ミュータブル操作とイミュータブル操作 #########\n",
    "\n",
    "# イミュータブル（不変）とミュータブル（可変）の操作の違いが関係しています。以下の例を使って詳細を説明します。\n",
    "\n",
    "# 例1: ミュータブル操作\n",
    "# df[col] = df[col].fillna(\"None\")は、データフレームの列を直接変更します。この場合、データフレームの列はミュータブルなオブジェクトであり、変更は元のデータフレームに反映されます。\n",
    "\n",
    "# 例2: イミュータブル操作\n",
    "# df = df.drop('Utilities', axis=1)は、元のデータフレームを変更せずに、新しいデータフレームを作成してdfに代入します。この場合、元のデータフレームは変更されません。\n",
    "\n",
    "# もう少し具体的に比較すると：\n",
    "\n",
    "# ミュータブル操作（列の変更）\n",
    "# for df in datasets:\n",
    "#     for col in cols_fillNone:\n",
    "#         df[col] = df[col].fillna(\"None\")\n",
    "# このループでは、df[col]はdatasets内の各データフレームの特定の列を参照しており、fillnaを適用してその列を更新しています。この変更はdatasetsの元のデータフレームに反映されます。\n",
    "\n",
    "# イミュータブル操作（データフレーム全体の置換）\n",
    "# for i in range(len(datasets)):\n",
    "#     datasets[i] = datasets[i].drop('Utilities', axis=1)\n",
    "# このループでは、dropメソッドは新しいデータフレームを生成します。datasets[i]をその新しいデータフレームに更新することで、元のデータフレームも更新されます。\n",
    "\n",
    "# したがって、以下のようにインデックスを使って元のデータフレームを更新すれば、Utilities列を削除する操作も期待通りに動作します。\n",
    "\n",
    "# for i in range(len(datasets)):\n",
    "#     if 'Utilities' in datasets[i].columns:\n",
    "#         datasets[i] = datasets[i].drop('Utilities', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4cf344",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [df_train, df_test]\n",
    "\n",
    "# LotFrontageの穴埋め\n",
    "for df in datasets:\n",
    "    df[\"LotFrontage\"] = df.apply(fillnaLot, axis=1)\n",
    "\n",
    "# Noneで穴埋めするもの\n",
    "cols_fillNone = [\n",
    "    \"PoolQC\",\n",
    "    \"MiscFeature\",\n",
    "    \"Alley\",\n",
    "    \"Fence\",\n",
    "    \"FireplaceQu\",\n",
    "    \"GarageCond\",\n",
    "    \"GarageQual\",\n",
    "    \"GarageFinish\",\n",
    "    \"GarageType\",\n",
    "    \"BsmtCond\",\n",
    "    \"BsmtExposure\",\n",
    "    \"BsmtQual\",\n",
    "    \"BsmtFinType1\",\n",
    "    \"BsmtFinType2\",\n",
    "]\n",
    "# 0で穴埋めするもの\n",
    "cols_fill0 = [\n",
    "    \"GarageYrBlt\",\n",
    "    \"MasVnrArea\",\n",
    "    \"BsmtFullBath\",\n",
    "    \"BsmtHalfBath\",\n",
    "    \"GarageCars\",\n",
    "    \"GarageArea\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"BsmtUnfSF\",\n",
    "    \"BsmtFinSF2\",\n",
    "    \"BsmtFinSF1\",\n",
    "]\n",
    "# 最頻値で穴埋めをするもの\n",
    "cols_fillmode = [\n",
    "    \"MasVnrType\",\n",
    "    \"MSZoning\",\n",
    "    \"Functional\",\n",
    "    \"Electrical\",\n",
    "    \"KitchenQual\",\n",
    "    \"Exterior2nd\",\n",
    "    \"Exterior1st\",\n",
    "    \"SaleType\",\n",
    "]\n",
    "\n",
    "# ミュータブルとイミュータブルな操作が混じってるけど、まとめて対応するために以下のfor文を構築した\n",
    "for i in range(len(datasets)):\n",
    "    for col in cols_fillNone:\n",
    "        datasets[i][col] = datasets[i][col].fillna(\"None\")\n",
    "\n",
    "    for col in cols_fill0:\n",
    "        datasets[i][col] = datasets[i][col].fillna(0)\n",
    "\n",
    "    for col in cols_fillmode:\n",
    "        datasets[i][col] = datasets[i][col].fillna(datasets[i][col].mode()[0])\n",
    "\n",
    "    # UtilitiesはAllPubが99.9%であり分析に使えないので列を削除\n",
    "    if \"Utilities\" in datasets[i].columns:\n",
    "        datasets[i] = datasets[i].drop(\"Utilities\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a29721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値が補完できたことを確認するため、もう一度欠損値割合を出力する\n",
    "\n",
    "# df_train, df_testの内容を更新する\n",
    "df_train, df_test = datasets  # アンパック\n",
    "\n",
    "# DataFrameを結合\n",
    "df_all_data = pd.concat([df_train, df_test])\n",
    "\n",
    "# 欠損値の数を計算\n",
    "missing_values_count = df_all_data.isna().sum()\n",
    "missing_values_table = pd.DataFrame(\n",
    "    {\n",
    "        \"Missing_total\": missing_values_count,\n",
    "        \"Percent (%)\": round(missing_values_count / len(df_all_data) * 100, 2),\n",
    "    }\n",
    ")\n",
    "\n",
    "# 欠損値の割合順で並べ替え\n",
    "df_missing = missing_values_table[\n",
    "    missing_values_table[\"Missing_total\"] > 0\n",
    "].sort_values(by=\"Missing_total\", ascending=False)\n",
    "\n",
    "display(df_missing)\n",
    "\n",
    "# SalePrice以外の欠損値が全て消えており、補完が上手く言ったことが分かる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f2460d",
   "metadata": {},
   "source": [
    "## 外れ値の取り扱い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd150c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GrLivArea\t地上（地上）の生活エリアの平方フィート。\n",
    "ax = sns.scatterplot(data=df_train, x=\"GrLivArea\", y=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0415da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 右下の2点は面積が広いのに価格がとても安い。恐らく郊外のエリアであり、予測モデルにおいては傾向を邪魔すると考えられるため除外する\n",
    "del_index = df_train.nlargest(2, \"GrLivArea\").index\n",
    "print(f\"before deleting: {df_train.shape}\")\n",
    "df_train = df_train.drop(del_index, axis=0)\n",
    "print(f\"after deleting: {df_train.shape}\")\n",
    "ax = sns.scatterplot(data=df_train, x=\"GrLivArea\", y=\"SalePrice\")\n",
    "# グラフからも、右下の外れ値が除去できていることが確認できる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8fabbb",
   "metadata": {},
   "source": [
    "## 特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5897573c",
   "metadata": {},
   "source": [
    "### 新しい特徴量'TotalSF'の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d56dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新しい特徴量（'TotalSF'：'TotalBsmtSF'、'1stFlrSF'、'2ndFlrSF'を合計したもの）の作成\n",
    "datasets = [df_train, df_test]\n",
    "\n",
    "for df in datasets:\n",
    "    print(f\"Before making: {df.shape}\")\n",
    "    df[\"TotalSF\"] = df[\"TotalBsmtSF\"] + df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]\n",
    "    print((f\"After making: {df.shape}\\n\"))\n",
    "\n",
    "# それぞれ、特徴量が1つずつ増えていることが分かる\n",
    "# 教材だとこの時点で特徴量が243個とかになっている…多分それ、one-hot encodingした後の値になってないかなぁ…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57177a5e",
   "metadata": {},
   "source": [
    "## 目的変数（SalePrice）の分布の調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5215588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_train[\"SalePrice\"]\n",
    "\n",
    "# 調整前の状態を再度確認\n",
    "print(\"-\" * 10, \"describe\", \"-\" * 10)\n",
    "print(round(data.describe(), 1))\n",
    "\n",
    "# 歪度と尖度\n",
    "skewness = data.skew()\n",
    "kurtosis = data.kurtosis()\n",
    "print(f\"歪度(Skewness: {skewness})\")\n",
    "print(f\"尖度(Kurtosis: {kurtosis})\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "sns.histplot(data, stat=\"density\", kde=True, ax=ax[0])\n",
    "ax[0].set_title(\"ヒストグラムと正規分布（調整前）\")\n",
    "ax[0].tick_params(axis=\"x\", labelsize=8)\n",
    "\n",
    "xmin, xmax = ax[0].get_xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = stats.norm.pdf(x, np.mean(data), np.std(data))\n",
    "ax[0].plot(x, p, \"k\", linewidth=1)\n",
    "\n",
    "res = stats.probplot(data, plot=ax[1])\n",
    "ax[1].set_title(\"正規確率プロット（調整前）\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1d23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SalePriceに対数変換を適用し、歪度・ヒストグラム・QQプロットを確認\n",
    "\n",
    "data = np.log(df_train[\"SalePrice\"])\n",
    "\n",
    "print(\"-\" * 10, \"describe\", \"-\" * 10)\n",
    "print(round(data.describe(), 1))\n",
    "\n",
    "skewness = data.skew()\n",
    "kurtosis = data.kurtosis()\n",
    "print(f\"歪度(Skewness: {skewness})\")\n",
    "print(f\"尖度(Kurtosis: {kurtosis})\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "sns.histplot(data, stat=\"density\", kde=True, ax=ax[0])\n",
    "ax[0].set_title(\"ヒストグラムと正規分布（調整後）\")\n",
    "ax[0].tick_params(axis=\"x\", labelsize=8)\n",
    "\n",
    "xmin, xmax = ax[0].get_xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = stats.norm.pdf(x, np.mean(data), np.std(data))\n",
    "ax[0].plot(x, p, \"k\", linewidth=1)\n",
    "\n",
    "res = stats.probplot(data, plot=ax[1])\n",
    "ax[1].set_title(\"正規確率プロット（調整後）\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 結果、良さそうなのでdf_train[\"SalePrice\"]を更新\n",
    "df_train[\"SalePrice\"] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a750104",
   "metadata": {},
   "source": [
    "## カテゴリカル変数のエンコーディング1\n",
    "順番が意味を成す特徴量について（マップベースのカテゴリーエンコーディング）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5388d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 順番が意味を成すデータ品質(*Qual)・状態(*Cond)等には、マップベースのカテゴリーエンコーディングを適用していきます\n",
    "# これめっちゃ面倒だけど…本当にこんなに手動でやってやらないといかんのか？特徴量が多いから面倒なのは仕方ないのか？\n",
    "# 面倒なのは、カテゴリ間に順序があるものとないもので分けてエンコーディングをしているため。全部one-hotエンコーディングするとどうなる？\n",
    "\n",
    "datasets = [df_train, df_test]\n",
    "# 以下では、ラベル付の種類ごとにfor文を作って回しているが、何かもっとスマートに書けないか？\n",
    "\n",
    "# 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC'について\n",
    "# これらは'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0でラベル付け\n",
    "label_mapping = {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"None\": 0}\n",
    "cols = [\n",
    "    \"ExterQual\",\n",
    "    \"ExterCond\",\n",
    "    \"BsmtQual\",\n",
    "    \"BsmtCond\",\n",
    "    \"HeatingQC\",\n",
    "    \"KitchenQual\",\n",
    "    \"FireplaceQu\",\n",
    "    \"GarageQual\",\n",
    "    \"GarageCond\",\n",
    "    \"PoolQC\",\n",
    "]\n",
    "for i in range(len(datasets)):\n",
    "    for col in cols:\n",
    "        datasets[i][col] = datasets[i][col].map(label_mapping).astype(int)\n",
    "\n",
    "# 'BsmtExposure'について\n",
    "# これらは'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0でラベル付け\n",
    "label_mapping = {\"Gd\": 4, \"Av\": 3, \"Mn\": 2, \"No\": 1, \"None\": 0}\n",
    "cols = [\"BsmtExposure\"]\n",
    "for i in range(len(datasets)):\n",
    "    for col in cols:\n",
    "        datasets[i][col] = datasets[i][col].map(label_mapping).astype(int)\n",
    "\n",
    "# 'GarageFinish'について\n",
    "# これらは'Fin': 3, 'RFn': 2, 'Unf': 1, 'None': 0でラベル付け\n",
    "label_mapping = {\"Fin\": 3, \"RFn\": 2, \"Unf\": 1, \"None\": 0}\n",
    "cols = [\"GarageFinish\"]\n",
    "for i in range(len(datasets)):\n",
    "    for col in cols:\n",
    "        datasets[i][col] = datasets[i][col].map(label_mapping).astype(int)\n",
    "\n",
    "# 'BsmtFinType1', 'BsmtFinType2'について\n",
    "# これらは'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0でラベル付け\n",
    "label_mapping = {\"GLQ\": 6, \"ALQ\": 5, \"BLQ\": 4, \"Rec\": 3, \"LwQ\": 2, \"Unf\": 1, \"None\": 0}\n",
    "cols = [\"BsmtFinType1\", \"BsmtFinType2\"]\n",
    "for i in range(len(datasets)):\n",
    "    for col in cols:\n",
    "        datasets[i][col] = datasets[i][col].map(label_mapping).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dfe5f7",
   "metadata": {},
   "source": [
    "## 各特徴量（説明変数）の分布の調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f9f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各特徴量の分布についても歪度調整を行う。目的変数SalePriceについては対数変換を行ったが、こっちではBox-Cox変換を行う\n",
    "\n",
    "# データ分析において、対数変換（`np.log`）と Box-Cox 変換（`scipy.special.boxcox1p`）は、データの分布を正規分布に近づけるために使用されるが、それぞれ異なる特徴と利点があります。\n",
    "\n",
    "# ### 対数変換（`np.log`）\n",
    "# - **方法**: 自然対数を取る変換。\n",
    "#   ```python\n",
    "#   np.log(x)\n",
    "#   ```\n",
    "# - **特性**:\n",
    "#   - 数値が正でなければならない（ゼロや負の数値に対しては定義されない）。\n",
    "#   - 変換後のデータが非常に小さい範囲になるため、元のデータのスケールが大きく影響する。\n",
    "#   - データの分布が正のスキューを持つ場合に有効。\n",
    "\n",
    "# ### Box-Cox 変換（`scipy.special.boxcox1p`）\n",
    "# - **方法**: パラメータ λ（lambda）に基づく、非線形なパワー変換。\n",
    "#   ```python\n",
    "#   from scipy.special import boxcox1p\n",
    "#   boxcox1p(x, λ)\n",
    "#   ```\n",
    "# - **特性**:\n",
    "#   - λ パラメータにより、変換の形状を調整できるため、データに応じて最適な変換を見つけやすい。\n",
    "#   - x が非負（ゼロを含む）であれば使用可能。\n",
    "#   - λ = 0 のとき、`boxcox1p(x, λ)` は `np.log1p(x)`（x+1 の対数変換）と同等。\n",
    "#   - より広い範囲のデータ分布を正規分布に近づけることができる。\n",
    "\n",
    "# ### 違いのまとめ\n",
    "# - **使用範囲**:\n",
    "#   - `np.log`: 正のデータに限定される。\n",
    "#   - `boxcox1p`: 非負のデータに対応。\n",
    "# - **柔軟性**:\n",
    "#   - `np.log`: 固定の変換方法。\n",
    "#   - `boxcox1p`: λ パラメータにより柔軟な調整が可能。\n",
    "# - **スキュー調整**:\n",
    "#   - `np.log`: 主に正のスキューを持つデータに対して効果的。\n",
    "#   - `boxcox1p`: λ の調整により、より多様な分布のデータに対応可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれの特徴量の歪度を確認する\n",
    "\n",
    "# データの結合\n",
    "ntrain = len(df_train)\n",
    "df_all_data = pd.concat([df_train, df_test])\n",
    "\n",
    "# 数値変数（カテゴリ変数ではなく）であるものを特定\n",
    "# https://note.nkmk.me/python-pandas-select-dtypes/#include pandas.DataFrameから特定の型の列を抽出・除外するselect_dtypesについて\n",
    "numeric_feats = df_all_data.select_dtypes(include=\"number\").columns\n",
    "\n",
    "# 各数値変数の歪度を計算し、降順にソート。そしてその中で特に歪度が大きい（abs > 0.75）ものを取り出す\n",
    "skew_bfBoxCox = (\n",
    "    df_all_data[numeric_feats]\n",
    "    .apply(lambda x: stats.skew(x.dropna()), axis=0)\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "skew_bfBoxCox = skew_bfBoxCox[abs(skew_bfBoxCox) > 0.75]\n",
    "\n",
    "print(\n",
    "    f\"There are {skew_bfBoxCox.shape[0]} skewed numerical features to Box Cox transform (abs > 0.75)\"\n",
    ")\n",
    "print(skew_bfBoxCox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef2e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-Cox変換をしていく\n",
    "skewed_index = skew_bfBoxCox.index\n",
    "lam = 0.15  # Box-Cox変換におけるパラメータλを設定（天下り式に教材の値を設定）→本来ここは各特徴量毎に適切なλを設定する必要あり？\n",
    "skew_aftBoxCox = pd.Series(index=skewed_index)\n",
    "\n",
    "# 各数値変数に対してBox-Cox変換を適用し、列の値を更新\n",
    "for idx in skewed_index:\n",
    "    df_all_data[idx] = special.boxcox1p(df_all_data[idx], lam)\n",
    "    skew_aftBoxCox[idx] = stats.skew(df_all_data[idx])\n",
    "\n",
    "pd.DataFrame({\"Skew bf Box-Cox\": skew_bfBoxCox, \"Skew aft Box-Cox\": skew_aftBoxCox})\n",
    "\n",
    "# Box-Cox変換を適用したことにより、概ね歪度が改善した\n",
    "# だが、一部Box-Cox変換によって歪度が増大しているものもある。明らかに不適切だが、とりあえず教材通りこのまま進める"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c933854",
   "metadata": {},
   "source": [
    "## カテゴリカル変数のエンコーディング2\n",
    "順番が意味を成さない特徴量について（One-hot encoding）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f16690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 現時点ではまだdf_trainとdf_testはdf_all_dataの値に更新されていない。なので初めはdf_all_dataを使いながら作業を進めていく\n",
    "\n",
    "# トレーニングデータの行数を取得\n",
    "ntrain = len(df_train)\n",
    "\n",
    "# one-hot encodingを実行\n",
    "df_all_data = pd.get_dummies(df_all_data)\n",
    "\n",
    "# df_all_dataを分割し、df_trainとdf_testに格納して中身を更新\n",
    "df_train = df_all_data[:ntrain]\n",
    "df_test = df_all_data[ntrain:]\n",
    "\n",
    "# これまでの操作により、df_testの方に\"SalePrice\"行が入ってしまっているのでこれを削除\n",
    "df_test = df_test.drop([\"SalePrice\"], axis=1)\n",
    "\n",
    "# データの様子を表示\n",
    "display(df_train)\n",
    "display(df_test)\n",
    "# 教材よりも行数が3つ少ない…何故？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083413e1",
   "metadata": {},
   "source": [
    "# 3. モデルの選択と構築"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3ed7a",
   "metadata": {},
   "source": [
    "## 使用する機械学習アルゴリズムの選定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回選定したアルゴリズムと特徴\n",
    "\n",
    "# LinearRegression: 線形回帰モデルは、線形関係を仮定し、特徴量と目的変数との線形関係を学習します。最小二乗法を使用してモデルを適合させ、回帰係数を推定します。特徴としては、線形関係がある場合に効果的です。\n",
    "# Ridge: リッジ回帰は線形回帰の一種で、L2正則化を使用して過学習を抑制することを特徴とします。正則化項は回帰係数の大きさを制約します。α（alpha）は正則化の強度を調整するハイパーパラメータです。\n",
    "# Lasso: ラッソ回帰は線形回帰の一種で、L1正則化を使用して過学習を抑制することを特徴とします。L1正則化は特徴量の選択を促進し、いくつかの特徴量を重要視する傾向があります。α（alpha）は正則化の強度を調整するハイパーパラメータです。\n",
    "# DecisionTreeRegressor: 決定木回帰は決定木をベースにした回帰モデルで、特徴量の値に基づいて目的変数を予測します。特徴としては、非線形な関係をキャプチャできる点があります。\n",
    "# RandomForestRegressor: ランダムフォレスト回帰は複数の決定木を組み合わせたアンサンブルモデルです。複数の決定木を使用することで、過学習を抑制し、高い予測性能を提供します。特徴としては、非線形関係のモデリングに有効です。\n",
    "# XGBRegressor: XGBoost回帰は勾配ブースティングモデルで、勾配ブースティングのアンサンブル技術を使用して、多数の決定木モデルを組み合わせて予測を行います。特徴としては、高い予測性能と汎化能力があります。\n",
    "# LGBMRegressor: LightGBM回帰も勾配ブースティングモデルの一種で、LightGBMライブラリを使用します。高速で効率的な勾配ブースティングアルゴリズムを提供し、大規模なデータセットに適しています。高速で正確な予測が特徴です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e74b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# 評価指標（コンペより引用）\n",
    "# 提出物は、予測値の対数と観測された販売価格の対数との間のRMSE（Root-Mean-Squared-Error）で評価される。(対数をとるということは、高価な住宅と安価な住宅の予測誤差が同じように結果に影響することを意味する)。\n",
    "\n",
    "# X: 説明変数、y: 目的変数\n",
    "X = df_train.drop([\"SalePrice\"], axis=1)\n",
    "y = df_train[\"SalePrice\"]\n",
    "\n",
    "# データ分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Pythonでは関数やオブジェクトをリストに入れることができます。ここでは、modelというリストに様々な回帰モデルのインスタンスを格納しています。\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(alpha=1.0),\n",
    "    Lasso(alpha=1.0),\n",
    "    DecisionTreeRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    XGBRegressor(),\n",
    "    LGBMRegressor(),\n",
    "]\n",
    "# 結果を保存するためのデータフレームを作成しておく\n",
    "results = pd.DataFrame(columns=[\"Model\", \"RMSE\"])\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    results = pd.concat(\n",
    "        [results, pd.DataFrame({\"Model\": [model_name], \"RMSE\": [rmse]})],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "results = results.sort_values(by=\"RMSE\")\n",
    "\n",
    "display(results)\n",
    "print(\"なお、目的変数の各統計量(y.describe())は以下の通り:\")\n",
    "display(y.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4428d42",
   "metadata": {},
   "source": [
    "## モデルのチューニング（グリッドサーチ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9160d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル選択にてリッジ回帰が一番RMSEが低い結果になったため、今度はRidge回帰の中でハイパーパラメータのalpha値を変えていき、RMSEが一番小さくなるalpha値を確認していく\n",
    "# グリッドサーチでは、指定したハイパーパラメータの候補値を組み合わせて全ての組み合わせを試し、最適な組み合わせを見つける\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 開始値（最小値）と終了値（最大値）を指定\n",
    "start_alpha = 0.01\n",
    "end_alpha = 100\n",
    "\n",
    "# 対数スケールでalphaの値を20個生成\n",
    "alphas = np.logspace(np.log10(start_alpha), np.log10(end_alpha), num=20)\n",
    "\n",
    "ridge = Ridge()\n",
    "\n",
    "# グリッドサーチのパラメータグリッドを定義\n",
    "param_grid = {\"alpha\": alphas}\n",
    "\n",
    "# クロスバリデーションとグリッドサーチを組み合わせて最適なalphaを探索\n",
    "grid_search = GridSearchCV(\n",
    "    ridge, param_grid, cv=5, scoring=\"neg_root_mean_squared_error\"\n",
    ")  # グリッドサーチの前準備\n",
    "grid_search.fit(\n",
    "    X_train, y_train\n",
    ")  # ここでグリッドサーチを実行。各αを設定したモデルについて学習をさせて、どのαを使ったときに最良の結果が得られるのか調べている\n",
    "\n",
    "best_alpha = grid_search.best_params_[\"alpha\"]\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Best score(negRMSE): {best_score}\")\n",
    "\n",
    "# チューニング前と比べて、RMSEが減少している（0.117716→0.113794）\n",
    "# このRMSEでRMSE/標準偏差を計算すると、=0.2847となりまあまあ小さい。つまりそこそこの精度がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適なalphaでRidge回帰モデルを再訓練\n",
    "model = Ridge(alpha=best_alpha).fit(X, y)\n",
    "\n",
    "# テストデータに対して予測を行い、exp(x) - 1を適用して元のスケールに戻す\n",
    "# 目的変数(SalePrice)の歪度調整のときに行った処理（data = np.log(df_train[\"SalePrice\"])）によって、テストデータの予測結果SalePriceはlog対数が適用されている。そのため、予測結果に対して元のスケールに戻す処理が必要\n",
    "# 疑問：対数変換するときに、log(y + 1)としていなかったのに、exp(x) - 1として良いのか？\n",
    "sub_pred = np.expm1(model.predict(df_test))\n",
    "submission = pd.DataFrame({\"Id\": df_test.index, \"SalePrice\": sub_pred})\n",
    "submission.to_csv(\"train_test_submission/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c994fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memo:他にやりたいこと\n",
    "# 特徴量重要度は？→ridge回帰だと正則化してるとかで？？\n",
    "\n",
    "feature_names = X_train.columns\n",
    "coefficients = abs(model.coef_)\n",
    "\n",
    "df_model_coef = pd.DataFrame({\"feature\": feature_names, \"coefficient\": coefficients})\n",
    "df_model_coef = df_model_coef.sort_values(by=\"coefficient\", ascending=False)\n",
    "\n",
    "display(df_model_coef.head(10))\n",
    "plt.figure(figsize=(40, 10))\n",
    "plt.bar(df_model_coef[\"feature\"], df_model_coef[\"coefficient\"])\n",
    "plt.xticks(fontsize=10, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb5e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
